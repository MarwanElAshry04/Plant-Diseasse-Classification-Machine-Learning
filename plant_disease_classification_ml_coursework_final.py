# -*- coding: utf-8 -*-
"""Plant Disease Classification - ML Coursework Final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kkKKWL58HopHJHhgbJBKe59DVfuPfzpm

## 1. Importing Necessary Libraries
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.metrics import ConfusionMatrixDisplay
from sklearn.metrics import roc_curve, roc_auc_score
from sklearn.metrics import precision_recall_curve
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.metrics import accuracy_score
from sklearn.metrics import roc_curve, auc

"""# 2. Loading Dataset"""

import kagglehub

# Download latest version
path = kagglehub.dataset_download("turakut/plant-disease-classification")

print("Path to dataset files:", path)

from pathlib import Path

dataset_path = kagglehub.dataset_download("turakut/plant-disease-classification")

dataset_directory = Path(dataset_path)

df = pd.read_csv(dataset_directory / "plant_disease_dataset.csv")

df.head()

"""# 3. Exploratory Data Analysis"""

print("Data Info: ")
df.info()

print("Missing values: ")
df.isnull().sum()

df.describe()

# Countplot showing the distribution of the disease presence.

sns.countplot(x = 'disease_present', data=df, palette='Set1')
plt.title("Disease Presence Distribution")
plt.show()

# Correlation map showing which features have the highest influence on diseases

plt.figure(figsize=(8,6))
sns.heatmap(df.corr(), annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Correlation Heatmap")
plt.show()

"""# 4. Unsupervised Learning - K-Means Clustering for feature engineering

4.1: Finding Optimal Number of Clusters
"""

feature_columns = ['temperature', 'humidity', 'rainfall', 'soil_pH']

X = df[feature_columns].copy()
y= df['disease_present']

# Scaing featurs for clustering
scaler_clustering = StandardScaler()
X_scaled = scaler_clustering.fit_transform(X)

# Finding optimal k using Elbow Method and Silhouette Score

inertias = []
silhouette_scores =[]
K_range = range(2, 8)

for k in K_range:
    kmeans_temp = KMeans(n_clusters=k, random_state=42, n_init=10)
    kmeans_temp.fit(X_scaled)
    inertias.append(kmeans_temp.inertia_)
    silhouette_scores.append(silhouette_score(X_scaled, kmeans_temp.labels_))

# Visualization: Elbow Method and Silhouette Score

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))

ax1.plot(K_range, inertias, 'bo-', linewidth=2, markersize=8)
ax1.set_xlabel('Number of clusters (k)', fontsize=12)
ax1.set_ylabel('Inertia', fontsize=12)
ax1.set_title('The Elbow Method', fontsize=14, fontweight = 'bold')
ax1.grid(True, alpha = 0.3)

ax2.plot(K_range, silhouette_scores, 'bo-', linewidth=2, markersize=8)
ax2.set_xlabel('Number of clusters (k)', fontsize=12)
ax2.set_ylabel('Silhouette Score', fontsize=12)
ax2.set_title('Silhouette Score', fontsize=14, fontweight = 'bold')
ax2.grid(True, alpha = 0.3)

plt.tight_layout()
plt.show()

"""4.2: Apply K-Means with optimal k"""

# Apply K-Means with optimal k based on elbow plot which shows k = 4 is best

optimal_k = 4

kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)
cluster_labels = kmeans.fit_predict(X_scaled)

print(f"Silhouette Score with k = 4: {silhouette_score(X_scaled, cluster_labels):.4f}")

# Visualization: Cluster results and disease distribution

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))

scatter = ax1.scatter(X_scaled[:, 0], X_scaled[:, 1], c=cluster_labels, cmap='viridis', alpha = 0.6, s=50)
ax1.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],
           c='red', marker='X', s=200, edgecolors='black', linewidths=2, label='Centroids')
ax1.set_xlabel('Temperature (scaled)')
ax1.set_ylabel('Humidity (scaled)')
ax1.set_title('K-Means Clustering Results')
ax1.legend()
ax1.grid(True, alpha=0.3)
plt.colorbar(scatter, ax=ax1, label='Cluster')

# Disease prevalence by cluster
cluster_disease_df = pd.DataFrame({'cluster': cluster_labels, 'disease_present': y})
cluster_disease_crosstab = pd.crosstab(cluster_disease_df['cluster'],
                                        cluster_disease_df['disease_present'],
                                        normalize='index') * 100
sns.heatmap(cluster_disease_crosstab, annot=True, fmt='.1f', cmap='RdYlGn_r', ax=ax2,
            cbar_kws={'label': '% of samples'})
ax2.set_title('Disease Prevalence by Cluster (%)')
ax2.set_xlabel('Disease Present')
ax2.set_ylabel('Cluster')

plt.tight_layout()
plt.show()

print("\nDisease distribution by cluster:")
print(cluster_disease_crosstab)

"""4.3: Creating cluster based features"""

# Feature 1: One-hot encoded cluster assignment

for i in range(optimal_k):
    X[f'cluster_{i}'] = (cluster_labels == i).astype(int)

# Feature 2: Distance to nearest cluster center

distances_to_centers = kmeans.transform(X_scaled)
X['distance_to_center'] = np.min(distances_to_centers, axis=1)

# Feature 3: Cluster density (typicality measure)

X['cluster_density'] = 1 / (1 + X['distance_to_center'])

print(f"Original features: {len(feature_columns)}")
print(f"Total features after clustering: {X.shape[1]}")
print(f"New features added: {X.shape[1] - len(feature_columns)}")

"""4.4: Preparing Datasets for Comparison"""

# Dataset 1: Original features only

X_original = X[feature_columns].copy()

# Dataset 2: Original and cluster features

cluster_cols = [col for col in X.columns if col not in feature_columns]
X_enhanced = X[feature_columns + cluster_cols].copy()

print("Original Dataset: ", X_original.shape[1], 'features')
print("Enhanced Dataset:", X_enhanced.shape[1], 'features')

# Split datasets

X_train_orig, X_test_orig, y_train, y_test = train_test_split(X_original, y, test_size=0.2, random_state=42)
X_train_enh, X_test_enh, _, _ = train_test_split(X_enhanced, y, test_size=0.2, random_state=42)

# Scaling datasets

scaler_orig = StandardScaler()
X_train_orig_scaled = scaler_orig.fit_transform(X_train_orig)
X_test_orig_scaled = scaler_orig.transform(X_test_orig)

scaler_enh = StandardScaler()
X_train_enh_scaled = scaler_enh.fit_transform(X_train_enh)
X_test_enh_scaled = scaler_enh.transform(X_test_enh)

"""# 5. Training Models with Original Features


*   Logistic Regression
*   Random Forest
*   XGBoost
*   LGBoosting
*   KNN






"""

# ==================== LOGISTIC REGRESSION ====================
logreg_orig = LogisticRegression(max_iter=500, class_weight='balanced', C=0.1, random_state=42)
logreg_orig.fit(X_train_orig_scaled, y_train)

logreg_orig_train_pred = logreg_orig.predict(X_train_orig_scaled)
logreg_orig_test_pred = logreg_orig.predict(X_test_orig_scaled)

logreg_orig_train_accuracy = accuracy_score(y_train, logreg_orig_train_pred)
logreg_orig_test_accuracy = accuracy_score(y_test, logreg_orig_test_pred)

print("Logistic Regression Test Accuracy:", logreg_orig_test_accuracy)
print("\nClassification Report:\n")
print(classification_report(y_test, logreg_orig_test_pred))

# ****************** RANDOM FOREST ***********************

rf_orig = RandomForestClassifier(
    n_estimators=300,
    max_depth=None,
    class_weight='balanced',
    random_state=42
)

rf_orig.fit(X_train_orig, y_train)

rf_orig_train_pred = rf_orig.predict(X_train_orig)
rf_orig_test_pred = rf_orig.predict(X_test_orig)

rf_orig_train_accuracy = accuracy_score(y_train, rf_orig_train_pred)
rf_orig_test_accuracy = accuracy_score(y_test, rf_orig_test_pred)

print("Random Forest Train Accuracy:", rf_orig_train_accuracy)
print("Random Forest Test Accuracy:", rf_orig_test_accuracy)
print("\nRF Classification Report:\n", classification_report(y_test, rf_orig_test_pred))

################### XGBoost ######################

neg_count = (y_train == 0).sum()
pos_count = (y_train == 1).sum()
ratio = neg_count / pos_count

xgb_orig = XGBClassifier(
    n_estimators=300,
    learning_rate=0.05,
    max_depth=4,
    subsample=0.8,
    colsample_bytree=0.8,
    scale_pos_weight=ratio,
    eval_metric='logloss',
    random_state=42
)

xgb_orig.fit(X_train_orig, y_train)

xgb_orig_train_pred = xgb_orig.predict(X_train_orig)
xgb_orig_test_pred = xgb_orig.predict(X_test_orig)

xgb_orig_train_accuracy = accuracy_score(y_train, xgb_orig_train_pred)
xgb_orig_test_accuracy = accuracy_score(y_test, xgb_orig_test_pred)

print("XGB Train Accuracy:", xgb_orig_train_accuracy)
print("XGB Test Accuracy:", xgb_orig_test_accuracy)
print("\nXGB Classification Report:\n", classification_report(y_test, xgb_orig_test_pred))

# ******************* LGBoosting **********************

lgb_orig = LGBMClassifier(
    n_estimators=300,
    learning_rate=0.05,
    max_depth=-1,
    num_leaves=31,
    subsample=0.8,
    colsample_bytree=0.8,
    class_weight={0:1, 1:ratio},
    random_state=42
)

lgb_orig.fit(X_train_orig, y_train)

lgb_orig_train_pred = lgb_orig.predict(X_train_orig)
lgb_orig_test_pred = lgb_orig.predict(X_test_orig)

lgb_orig_train_accuracy = accuracy_score(y_train, lgb_orig_train_pred)
lgb_orig_test_accuracy = accuracy_score(y_test, lgb_orig_test_pred)

print("LGBM Train Accuracy:", lgb_orig_train_accuracy)
print("LGBM Test Accuracy:", lgb_orig_test_accuracy)
print("\nLGBM Classification Report:\n", classification_report(y_test, lgb_orig_test_pred))

# ******************** KNN *************************

knn_orig = KNeighborsClassifier(n_neighbors=7)

knn_orig.fit(X_train_orig_scaled, y_train)

knn_orig_train_pred = knn_orig.predict(X_train_orig_scaled)
knn_orig_test_pred = knn_orig.predict(X_test_orig_scaled)

knn_orig_train_accuracy = accuracy_score(y_train, knn_orig_train_pred)
knn_orig_test_accuracy = accuracy_score(y_test, knn_orig_test_pred)

print("KNN Train Accuracy:", knn_orig_train_accuracy)
print("KNN Test Accuracy:", knn_orig_test_accuracy)
print("\nKNN Classification Report:\n", classification_report(y_test, knn_orig_test_pred))

"""# 6. Training Models with Enhanced Features (with clustering)"""

# **************** Logistic Regression (Enhanced) ***********************

logreg_enh = LogisticRegression(max_iter=500, class_weight='balanced', C=0.1, random_state=42)
logreg_enh.fit(X_train_enh_scaled, y_train)

logreg_enh_train_pred = logreg_enh.predict(X_train_enh_scaled)
logreg_enh_test_pred = logreg_enh.predict(X_test_enh_scaled)

logreg_enh_train_accuracy = accuracy_score(y_train, logreg_enh_train_pred)
logreg_enh_test_accuracy = accuracy_score(y_test, logreg_enh_test_pred)

print("Logistic Regression Test Accuracy:", logreg_enh_test_accuracy)
print("\nClassification Report:\n")
print(classification_report(y_test, logreg_enh_test_pred))

# ********************** Random Forest (Enhanced) ******************************

rf_enh = RandomForestClassifier(
    n_estimators=300,
    max_depth=None,
    class_weight='balanced',
    random_state=42
)

rf_enh.fit(X_train_enh, y_train)

rf_enh_train_pred = rf_enh.predict(X_train_enh)
rf_enh_test_pred = rf_enh.predict(X_test_enh)

rf_enh_train_accuracy = accuracy_score(y_train, rf_enh_train_pred)
rf_enh_test_accuracy = accuracy_score(y_test, rf_enh_test_pred)

print("Random Forest Train Accuracy:", rf_enh_train_accuracy)
print("Random Forest Test Accuracy:", rf_enh_test_accuracy)
print("\nRF Classification Report:\n", classification_report(y_test, rf_enh_test_pred))

# *********************** XGB (Enhanced) ****************************

xgb_enh = XGBClassifier(
    n_estimators=300,
    learning_rate=0.05,
    max_depth=4,
    subsample=0.8,
    colsample_bytree=0.8,
    scale_pos_weight=ratio,
    eval_metric='logloss',
    random_state=42
)

xgb_enh.fit(X_train_enh, y_train)

xgb_enh_train_pred = xgb_enh.predict(X_train_enh)
xgb_enh_test_pred = xgb_enh.predict(X_test_enh)

xgb_enh_train_accuracy = accuracy_score(y_train, xgb_enh_train_pred)
xgb_enh_test_accuracy = accuracy_score(y_test, xgb_enh_test_pred)

print("XGB Train Accuracy:", xgb_enh_train_accuracy)
print("XGB Test Accuracy:", xgb_enh_test_accuracy)
print("\nXGB Classification Report:\n", classification_report(y_test, xgb_enh_test_pred))

# **************** LGBoosting (Enhanced) ************************

lgb_enh = LGBMClassifier(
    n_estimators=300,
    learning_rate=0.05,
    max_depth=-1,
    num_leaves=31,
    subsample=0.8,
    colsample_bytree=0.8,
    class_weight={0:1, 1:ratio},
    random_state=42
)

lgb_enh.fit(X_train_enh, y_train)

lgb_enh_train_pred = lgb_enh.predict(X_train_enh)
lgb_enh_test_pred = lgb_enh.predict(X_test_enh)

lgb_enh_train_accuracy = accuracy_score(y_train, lgb_enh_train_pred)
lgb_enh_test_accuracy = accuracy_score(y_test, lgb_enh_test_pred)

print("LGBM Train Accuracy:", lgb_enh_train_accuracy)
print("LGBM Test Accuracy:", lgb_enh_test_accuracy)
print("\nLGBM Classification Report:\n", classification_report(y_test, lgb_enh_test_pred))

# ***************** KNN (enhanced) ********************

knn_enh = KNeighborsClassifier(n_neighbors=7)

knn_enh.fit(X_train_enh_scaled, y_train)

knn_enh_train_pred = knn_enh.predict(X_train_enh_scaled)
knn_enh_test_pred = knn_enh.predict(X_test_enh_scaled)

knn_enh_train_accuracy = accuracy_score(y_train, knn_enh_train_pred)
knn_enh_test_accuracy = accuracy_score(y_test, knn_enh_test_pred)

print("KNN Train Accuracy:", knn_enh_train_accuracy)
print("KNN Test Accuracy:", knn_enh_test_accuracy)
print("\nKNN Classification Report:\n", classification_report(y_test, knn_enh_test_pred))

"""# 7. Results Comparison and Visualization"""

"""# 7. Comparing Original vs Enhanced Features"""

models = ['LogReg', 'Random Forest', 'XGBoost', 'LightGBM', 'KNN']

orig_train_accs = [
    logreg_orig_train_accuracy,
    rf_orig_train_accuracy,
    xgb_orig_train_accuracy,
    lgb_orig_train_accuracy,
    knn_orig_train_accuracy
]

orig_test_accs = [
    logreg_orig_test_accuracy,
    rf_orig_test_accuracy,
    xgb_orig_test_accuracy,
    lgb_orig_test_accuracy,
    knn_orig_test_accuracy
]

enh_train_accs = [
    logreg_enh_train_accuracy,
    rf_enh_train_accuracy,
    xgb_enh_train_accuracy,
    lgb_enh_train_accuracy,
    knn_enh_train_accuracy
]

enh_test_accs = [
    logreg_enh_test_accuracy,
    rf_enh_test_accuracy,
    xgb_enh_test_accuracy,
    lgb_enh_test_accuracy,
    knn_enh_test_accuracy
]

# Calculate improvements
improvements = [((enh - orig) / orig) * 100 for orig, enh in zip(orig_test_accs, enh_test_accs)]

# Create comparison plot
x = np.arange(len(models))
width = 0.35

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))

# Accuracy comparison
ax1.bar(x - width/2, orig_test_accs, width, label='Original Features', alpha=0.8, color='skyblue')
ax1.bar(x + width/2, enh_test_accs, width, label='Enhanced Features', alpha=0.8, color='lightcoral')
ax1.set_ylabel('Test Accuracy')
ax1.set_title('Test Accuracy: Original vs Enhanced Features', fontweight='bold')
ax1.set_xticks(x)
ax1.set_xticklabels(models)
ax1.legend()
ax1.grid(True, alpha=0.3, axis='y')
ax1.set_ylim([0, 1])

# Add value labels
for i, (orig, enh) in enumerate(zip(orig_test_accs, enh_test_accs)):
    ax1.text(i - width/2, orig + 0.01, f'{orig:.3f}', ha='center', va='bottom', fontsize=9)
    ax1.text(i + width/2, enh + 0.01, f'{enh:.3f}', ha='center', va='bottom', fontsize=9)

# Improvement percentages
colors = ['green' if imp > 0 else 'red' for imp in improvements]
ax2.barh(models, improvements, color=colors, alpha=0.7)
ax2.set_xlabel('% Improvement')
ax2.set_title('Percentage Improvement from Feature Engineering', fontweight='bold')
ax2.axvline(x=0, color='black', linestyle='-', linewidth=0.8)
ax2.grid(True, alpha=0.3, axis='x')

for i, imp in enumerate(improvements):
    ax2.text(imp, i, f' {imp:+.2f}%', va='center', fontsize=10, fontweight='bold')

plt.tight_layout()
plt.show()

"""# 8. Evaluating the Best Model (Random Forest)"""

# Visualization: Confusion matrix for the random forest enhanced model

cm = confusion_matrix(y_test, rf_enh_test_pred)
plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['No Disease', 'Disease'],
            yticklabels=['No Disease', 'Disease'])
plt.ylabel('Actual')
plt.xlabel('Predicted')
plt.title(f'Confusion Matrix - Random Forest (Enhanced)')
plt.show()

importance = rf_enh.feature_importances_
features = X_train_enh.columns

feature_imp_df = pd.DataFrame({
    'Feature': features,
    'Importance': importance
}).sort_values('Importance', ascending=False)

plt.figure(figsize=(10, 6))
top_features = feature_imp_df.head(10)
sns.barplot(data=top_features, y='Feature', x='Importance', orient='h', palette='viridis')
plt.title('Top 10 Feature Importances - Random Forest (Enhanced)')
plt.xlabel('Importance Score')
plt.ylabel('Feature')
plt.grid(True, alpha=0.3, axis='x')
plt.show()

# Print summary
print("IMPROVEMENT SUMMARY")

for model, orig, enh, imp in zip(models, orig_test_accs, enh_test_accs, improvements):
    print(f"{model:15s}: {orig:.4f} â†’ {enh:.4f} ({imp:+.2f}%)")
print(f"Average improvement: {np.mean(improvements):+.2f}%")